{"meta":{"title":"Pengyun's Blog","subtitle":"ლ(ٱ٥ٱლ)","description":"学习的路上，不时写点总结","author":"pengyun","url":"https://pyun.top","root":"/"},"pages":[{"title":"分类","date":"2019-03-26T07:08:55.000Z","updated":"2019-03-26T09:51:42.337Z","comments":true,"path":"categories/index.html","permalink":"https://pyun.top/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-03-26T09:12:11.232Z","updated":"2019-03-26T09:12:11.232Z","comments":false,"path":"tags/index.html","permalink":"https://pyun.top/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"源码阅读 - Hyperf数据库连接管理","slug":"hyperf-db-connection","date":"2020-10-17T11:45:42.000Z","updated":"2020-10-18T00:16:37.674Z","comments":true,"path":"2020/hyperf-db-connection/","link":"","permalink":"https://pyun.top/2020/hyperf-db-connection/","excerpt":"","text":"加载容器- bin/hyperf.php 1require BASE_PATH . '/config/container.php' - vendor/hyperf/di/src/Definition/DefinitionSourceFactory.php:46 1ProviderConfig::load() 寻找服务提供者配置- vendor/hyperf/config/src/ProviderConfig.php:37 123456789101112131415161718192021222324252627282930313233343536373839404142434445Composer::getMergedExtra('hyperf')['config'] ?? []``` 找的时候就去composer.lock读几个特定的键（恭喜composer.lock有了额外功能）``` php if (! $path) &#123; throw new \\RuntimeException('composer.lock not found.'); &#125; self::$content = collect(json_decode(file_get_contents($path), true)); $packages = self::$content-&gt;offsetGet('packages') ?? []; $packagesDev = self::$content-&gt;offsetGet('packages-dev') ?? []; foreach (array_merge($packages, $packagesDev) as $package) &#123; $packageName = ''; foreach ($package ?? [] as $key =&gt; $value) &#123; if ($key === 'name') &#123; $packageName = $value; continue; &#125; switch ($key) &#123; case 'extra': $packageName &amp;&amp; self::$extra[$packageName] = $value; break; case 'scripts': $packageName &amp;&amp; self::$scripts[$packageName] = $value; break; case 'version': $packageName &amp;&amp; self::$versions[$packageName] = $value; break; &#125; &#125; &#125;``` composer.lock 配合一下``` json &#123; \"name\": \"hyperf/db-connection\", \"extra\": &#123; \"branch-alias\": &#123; \"dev-master\": \"1.1-dev\" &#125;, \"hyperf\": &#123; \"config\": \"Hyperf\\\\DbConnection\\\\ConfigProvider\" &#125; &#125;, &#125;, 结果 123456789[ 0 =&gt; \"Hyperf\\AsyncQueue\\ConfigProvider\" 1 =&gt; \"Hyperf\\Cache\\ConfigProvider\" 2 =&gt; \"Hyperf\\Config\\ConfigProvider\" 3 =&gt; \"Hyperf\\Constants\\ConfigProvider\" 4 =&gt; \"Hyperf\\Crontab\\ConfigProvider\" 5 =&gt; \"Hyperf\\DbConnection\\ConfigProvider\" // 省略...] DB相关配置 1234567891011121314151617181920use Hyperf\\Database\\ConnectionResolverInterface;use Hyperf\\Database\\Connectors\\ConnectionFactory;use Hyperf\\Database\\Connectors\\MySqlConnector;use Hyperf\\Database\\Migrations\\MigrationRepositoryInterface;use Hyperf\\DbConnection\\Listener\\RegisterConnectionResolverListener;use Hyperf\\DbConnection\\Pool\\PoolFactory;[ 'dependencies' =&gt; [ PoolFactory::class =&gt; PoolFactory::class, ConnectionFactory::class =&gt; ConnectionFactory::class, ConnectionResolverInterface::class =&gt; ConnectionResolver::class, 'db.connector.mysql' =&gt; MySqlConnector::class, MigrationRepositoryInterface::class =&gt; DatabaseMigrationRepositoryFactory::class, ], 'listeners' =&gt; [ RegisterConnectionResolverListener::class, ], // ...]; 应用启动时初始化数据库连接解析器 Register::setConnectionResolvervendor/hyperf/db-connection/src/Listener/RegisterConnectionResolverListener.php:32 123456789101112131415public function listen(): array&#123; return [ BootApplication::class, ];&#125;public function process(object $event)&#123; if ($this-&gt;container-&gt;has(ConnectionResolverInterface::class)) &#123; Register::setConnectionResolver( $this-&gt;container-&gt;get(ConnectionResolverInterface::class) ); &#125;&#125; 在Model上调用Query时，通过一连串调用，初始化一堆东西，包括数据库连接vendor/hyperf/database/src/Model/Model.php:939 1234567891011121314151617181920212223242526272829303132public static function query()&#123; return (new static())-&gt;newQuery();&#125;public function newQuery()&#123; return $this-&gt;registerGlobalScopes($this-&gt;newQueryWithoutScopes());&#125;public function newQueryWithoutScopes()&#123; return $this-&gt;newModelQuery()-&gt;with($this-&gt;with)-&gt;withCount($this-&gt;withCount);&#125;public function newModelQuery()&#123; return $this-&gt;newModelBuilder($this-&gt;newBaseQueryBuilder())-&gt;setModel($this);&#125;protected function newBaseQueryBuilder()&#123; $connection = $this-&gt;getConnection(); return new QueryBuilder($connection, $connection-&gt;getQueryGrammar(), $connection-&gt;getPostProcessor());&#125;// 这里初始化数据库连接public function getConnection(): ConnectionInterface&#123; return Register::resolveConnection($this-&gt;getConnectionName());&#125; 初始化的具体步骤，由上面第3步的解析器包办 vendor/hyperf/database/src/Model/Register.php:38 1234public static function resolveConnection($connection = null)&#123; return static::$resolver-&gt;connection($connection);&#125; 上面说的是 `Hyperf\\Database\\Model\\Model` 的实现，而 `Hyperf\\DbConnection\\Model\\Model` 继承了它，并且覆盖 `getConnection` 方法，变成了 vendor/hyperf/db-connection/src/Model/Model.php:32 123456public function getConnection(): ConnectionInterface&#123; $connectionName = $this-&gt;getConnectionName(); $resolver = $this-&gt;getContainer()-&gt;get(ConnectionResolverInterface::class); return $resolver-&gt;connection($connectionName);&#125; 虽然做的事跟基类是一样的，不过变成了自己一把梭，第3步的解析器被无视了 实际获取一个数据库连接vendor/hyperf/db-connection/src/ConnectionResolver.php:52 12345678910111213141516171819202122232425262728293031public function connection($name = null)&#123; if (is_null($name)) &#123; $name = $this-&gt;getDefaultConnection(); &#125; $connection = null; $id = $this-&gt;getContextKey($name); if (Context::has($id)) &#123; $connection = Context::get($id); &#125; if (! $connection instanceof ConnectionInterface) &#123; $pool = $this-&gt;factory-&gt;getPool($name); $connection = $pool-&gt;get(); // 获取Hyperf\\DbConnection\\Connection类 try &#123; // PDO is initialized as an anonymous function, so there is no IO exception, // but if other exceptions are thrown, the connection will not return to the connection pool properly. $connection = $connection-&gt;getConnection(); // 检查连接是否超时，超时则重连 Context::set($id, $connection); // 保存当前协程所用的连接 &#125; finally &#123; if (Coroutine::inCoroutine()) &#123; defer(function () use ($connection) &#123; $connection-&gt;release(); &#125;); &#125; &#125; &#125; return $connection;&#125; vendor/hyperf/pool/src/Pool.php:58 123456789101112131415161718192021222324252627282930313233343536373839404142public function get(): ConnectionInterface&#123; $connection = $this-&gt;getConnection(); if ($this-&gt;frequency instanceof FrequencyInterface) &#123; $this-&gt;frequency-&gt;hit(); // 记录一次从池子中获取连接 &#125; if ($this-&gt;frequency instanceof LowFrequencyInterface) &#123; if ($this-&gt;frequency-&gt;isLowFrequency()) &#123; $this-&gt;flush(); // 获取连接频率不高时，释放一些连接 &#125; &#125; return $connection;&#125;private function getConnection(): ConnectionInterface&#123; // 获取池子中连接 $num = $this-&gt;getConnectionsInChannel(); try &#123; // 没有空闲连接且连接数没达到上限时，创建连接 if ($num === 0 &amp;&amp; $this-&gt;currentConnections &lt; $this-&gt;option-&gt;getMaxConnections()) &#123; ++$this-&gt;currentConnections; return $this-&gt;createConnection(); &#125; &#125; catch (Throwable $throwable) &#123; --$this-&gt;currentConnections; throw $throwable; &#125; // 从存储中pop出一个连接 $connection = $this-&gt;channel-&gt;pop($this-&gt;option-&gt;getWaitTimeout()); return $connection;&#125;protected function createConnection(): ConnectionInterface&#123; return new Connection($this-&gt;container, $this, $this-&gt;config);&#125; vendor/hyperf/db-connection/src/Connection.php:84 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public function getActiveConnection(): DbConnectionInterface&#123; if ($this-&gt;check()) &#123; return $this; &#125; if (! $this-&gt;reconnect()) &#123; throw new ConnectionException('Connection reconnect failed.'); &#125; return $this;&#125;// 连接上次使用时间，跟配置的超时时间对比，作为超时判断public function check(): bool&#123; $maxIdleTime = $this-&gt;pool-&gt;getOption()-&gt;getMaxIdleTime(); $now = microtime(true); if ($now &gt; $maxIdleTime + $this-&gt;lastUseTime) &#123; return false; &#125; $this-&gt;lastUseTime = $now; return true;&#125;public function reconnect(): bool&#123; // 这个connection包含了连到数据库的闭包，在真实操作数据库时执行 $this-&gt;connection = $this-&gt;factory-&gt;make($this-&gt;config); if ($this-&gt;connection instanceof \\Hyperf\\Database\\Connection) &#123; // Reset event dispatcher after db reconnect. if ($this-&gt;container-&gt;has(EventDispatcherInterface::class)) &#123; $dispatcher = $this-&gt;container-&gt;get(EventDispatcherInterface::class); $this-&gt;connection-&gt;setEventDispatcher($dispatcher); &#125; // Reset reconnector after db reconnect. // 重连器，是个重连数据库的闭包，如下所见，重连器会重新生成上面的连接闭包，并执行它来实现重连 $this-&gt;connection-&gt;setReconnector(function ($connection) &#123; $this-&gt;refresh($connection); &#125;); &#125; $this-&gt;lastUseTime = microtime(true); return true;&#125;protected function refresh(\\Hyperf\\Database\\Connection $connection)&#123; $refresh = $this-&gt;factory-&gt;make($this-&gt;config); if ($refresh instanceof \\Hyperf\\Database\\Connection) &#123; $connection-&gt;disconnect(); $connection-&gt;setPdo($refresh-&gt;getPdo()); $connection-&gt;setReadPdo($refresh-&gt;getReadPdo()); &#125; $this-&gt;logger-&gt;warning('Database connection refreshed.');&#125; vendor/hyperf/database/src/Connectors/ConnectionFactory.php:45 12345678910public function make(array $config, $name = null)&#123; $config = $this-&gt;parseConfig($config, $name); if (isset($config['read'])) &#123; return $this-&gt;createReadWriteConnection($config); &#125; return $this-&gt;createSingleConnection($config);&#125; 连接数据库的闭包长这样 vendor/hyperf/database/src/Connectors/ConnectionFactory.php:196 1234567891011121314151617181920212223242526272829303132333435363738protected function createPdoResolverWithHosts(array $config)&#123; return function () use ($config) &#123; foreach (Arr::shuffle($hosts = $this-&gt;parseHosts($config)) as $key =&gt; $host) &#123; $config['host'] = $host; try &#123; return $this-&gt;createConnector($config)-&gt;connect($config); &#125; catch (PDOException $e) &#123; continue; &#125; &#125; throw $e; &#125;;&#125;/** * Parse the hosts configuration item into an array. * * @return array */protected function parseHosts(array $config)&#123; return Arr::wrap($config['host']);&#125;/** * Create a new Closure that resolves to a PDO instance where there is no configured host. * * @return \\Closure */protected function createPdoResolverWithoutHosts(array $config)&#123; return function () use ($config) &#123; return $this-&gt;createConnector($config)-&gt;connect($config); &#125;;&#125; 闭包里的`Connector`是这样连数据库的 vendor/hyperf/database/src/Connectors/Connector.php:105 12345678protected function createPdoConnection($dsn, $username, $password, $options)&#123; if (class_exists(PDOConnection::class) &amp;&amp; ! $this-&gt;isPersistentConnection($options)) &#123; return new PDOConnection($dsn, $username, $password, $options); &#125; return new PDO($dsn, $username, $password, $options);&#125; 真实执行数据库操作，比如select时，会通过pdo属性的值获取连接，此时如果pdo还是个闭包，就会执行闭包，建立到数据库的网络连接，并且更新把pdo的值从闭包更新成 `PDO` 对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113public function select(string $query, array $bindings = [], bool $useReadPdo = true): array&#123; return $this-&gt;run($query, $bindings, function ($query, $bindings) use ($useReadPdo) &#123; if ($this-&gt;pretending()) &#123; return []; &#125; // For select statements, we'll simply execute the query and return an array // of the database result set. Each element in the array will be a single // row from the database table, and will either be an array or objects. $statement = $this-&gt;prepared($this-&gt;getPdoForSelect($useReadPdo) -&gt;prepare($query)); $this-&gt;bindValues($statement, $this-&gt;prepareBindings($bindings)); $statement-&gt;execute(); return $statement-&gt;fetchAll(); &#125;);&#125;public function getPdo()&#123; if ($this-&gt;pdo instanceof Closure) &#123; return $this-&gt;pdo = call_user_func($this-&gt;pdo); &#125; return $this-&gt;pdo;&#125;/** * Get the current PDO connection used for reading. * * @return \\PDO */public function getReadPdo()&#123; if ($this-&gt;transactions &gt; 0) &#123; return $this-&gt;getPdo(); &#125; if ($this-&gt;recordsModified &amp;&amp; $this-&gt;getConfig('sticky')) &#123; return $this-&gt;getPdo(); &#125; if ($this-&gt;readPdo instanceof Closure) &#123; return $this-&gt;readPdo = call_user_func($this-&gt;readPdo); &#125; return $this-&gt;readPdo ?: $this-&gt;getPdo();&#125;protected function run($query, $bindings, Closure $callback)&#123; // 这里会判断PDO是否是空的 $this-&gt;reconnectIfMissingConnection(); // Here we will run this query. If an exception occurs we'll determine if it was // caused by a connection that has been lost. If that is the cause, we'll try // to re-establish connection and re-run the query with a fresh connection. try &#123; $result = $this-&gt;runQueryCallback($query, $bindings, $callback); &#125; catch (QueryException $e) &#123; // 这里也会判断是不是连接失败引起的异常，是的话也会重连 $result = $this-&gt;handleQueryException( $e, $query, $bindings, $callback ); &#125; return $result;&#125;protected function handleQueryException($e, $query, $bindings, Closure $callback)&#123; // 省略... return $this-&gt;tryAgainIfCausedByLostConnection( $e, $query, $bindings, $callback );&#125;/** * Handle a query exception that occurred during query execution. * * @param string $query * @param array $bindings * @throws QueryException */protected function tryAgainIfCausedByLostConnection(QueryException $e, $query, $bindings, Closure $callback)&#123; if ($this-&gt;causedByLostConnection($e-&gt;getPrevious())) &#123; $this-&gt;reconnect(); return $this-&gt;runQueryCallback($query, $bindings, $callback); &#125; throw $e;&#125;/** * Reconnect to the database if a PDO connection is missing. */protected function reconnectIfMissingConnection()&#123; if (is_null($this-&gt;pdo)) &#123; $this-&gt;reconnect(); &#125;&#125;","categories":[],"tags":[]},{"title":"Hive on Spark 搭建","slug":"hive-on-spark-deploy","date":"2019-06-10T13:59:43.000Z","updated":"2019-06-11T06:46:59.981Z","comments":true,"path":"2019/running-hive-on-spark/","link":"","permalink":"https://pyun.top/2019/running-hive-on-spark/","excerpt":"","text":"Hive on Spark 就是把 Hive 默认的执行引擎从 MapReduce 换成 Apache Spark。Hive on Spark 只在 Spark 的制定版本上进行了测试，其它版本则不是很有保证。对稳定性有要求可以去官方文档查看版本兼容性，再根据 Hive 和 Spark 源码中 pom 文件的配置，另行选择版本。这里几乎全部选择最新版进行演示。 环境及一些信息 VMWare 15，参考之前的搭建博文 Ubuntu 18.04 bionic 8G 内存 Docker 18.09.0 IP: 192.168.99.100，后面看到这个地址出现就自行翻译成你们运行环境的ip 全程 root 用户操作（手动滑稽） 准备 安装Docker，拉取 MySQL 8 镜像（不使用 Docker 的后面自行安装 MySQL） 下载安装包 AdoptOpenJDK 8 Maven 3.6.1 Hadoop 3.1.2 Hive 3.1.1 Scala 2.11.12 Mysql Connector 8.0.16 Spark 二进制包可以在这里获取，但是我们要自己编译不内建 Hive 的版本，所以要下载源码编译 12git clone -b v2.3.3 https://github.com/apache/spark.gitrm -rf spark/.git 创建 /home/tools 文件夹，将解压后的安装包和源码都放这里，解压后的 mysql-connector-java-8.0.16.jar 放到 /home/tools/hive/lib 12345678910home | -- tools | -- hadoop | -- hive | -- lib | -- mysql-connector-java-8.0.16.jar | -- java | -- maven | -- scala | -- spark-source 安装Java、Maven 和 Scala 安装 编辑 /etc/profile，加入环境变量 1234export JAVA_HOME=/home/tools/javaexport M2_HOME=/home/tools/mavenexport SCALA_HOME=/home/tools/scalaexport PATH=$&#123;M2_HOME&#125;/bin:$&#123;SCALA_HOME&#125;/bin:$&#123;JAVA_HOME&#125;/bin:$PATH 在终端运行以下指令，若显示出 jdk、Scala 和 Maven 版本，则安装已完成 1234source /etc/profilejava -versionscala -versionmvn -v （可选）网络不好的孩纸可自行配置 Maven 的仓库为阿里云的镜像 Hadoop 安装 安装需要的软件 1apt install -y ssh pdsh rsync 修改配置 etc/hadoop/core-site.xml: 12345678910&lt;configuration&gt; &lt;property&gt; name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/tmp/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; etc/hadoop/hdfs-site.xml: 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; etc/hadoop/mapred-site.xml: 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; etc/hadoop/yarn-site.xml: 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; sbin/start-dfs.sh、sbin/stop-dfs.sh 头部配置使用 root 账户: 12345#!/usr/bin/env bashHDFS_DATANODE_USER=rootHDFS_DATANODE_SECURE_USER=hdfsHDFS_NAMENODE_USER=rootHDFS_SECONDARYNAMENODE_USER=root sbin/start-yarn.sh、sbin/stop-yarn.sh 头部配置使用 root 账户: 1234#!/usr/bin/env bashYARN_RESOURCEMANAGER_USER=rootHADOOP_SECURE_DN_USER=yarnYARN_NODEMANAGER_USER=root etc/hadoop/hadoop-env.sh 中添加: 12export PDSH_RCMD_TYPE=sshexport JAVA_HOME=/home/tools/java 配置 SSH 密钥登录 123ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsacat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keyschmod 0600 ~/.ssh/authorized_keys 修改 SSH 配置文件 /etc/ssh/sshd_config，加入: 12PermitRootLogin yes PubkeyAuthentication yes 重启 SSH 服务: 1service ssh restart 运行 ssh localhost 测试，看是否直接登录到本机 格式化文件系统 1bin/hdfs namenode -format 运行 HDFS，运行成功后可访问 http://192.168.99.100:9870 查看 NameNode 的 Web 管理界面 1sbin/start-dfs.sh 运行 yarn，运行成功后可访问 http://192.168.99.100:8088 查看 ResourceManager 的 Web 管理界面 1sbin/start-yarn.sh Spark 安装 编译不带 Hive 的版本 12345cd /home/tools/spark-source./dev/make-distribution.sh --name \"hadoop3-without-hive\" --tgz \"-Pyarn,-Phadoop-3.1,-Dhadoop.version=3.1.2,parquet-provided,orc-provided\"cd ..mkdir sparktar -xf spark-source/spark-2.3.3-bin-hadoop3-without-hive.tgz --strip-components 1 -C spark 新建配置文件 /home/tools/spark/conf/spark-env.sh: 1234#!/usr/bin/env bashexport JAVA_HOME=/home/tools/javaexport HADOOP_CONF_DIR=/home/tools/hadoop/etc/hadoopexport SPARK_DIST_CLASSPATH=$(/home/tools/hadoop/bin/hadoop classpath) Hive 安装 复制 Jar 包到 Hive 的 lib 文件夹，其中某些包的小版本号可能会不一样 1234567891011121314cd /home/tools/hivecp ../scala/lib/scala-library.jar lib/cp ../spark/jars/spark-core_2.11-2.3.3.jar lib/cp ../spark/jars/spark-network-common_2.11-2.3.3.jar lib/cp ../spark/jars/jersey-container-servlet-core-2.22.2.jar lib/cp ../spark/jars/jersey-server-2.22.2.jar lib/cp ../spark/jars/json4s-ast_2.11-3.2.11.jar lib/cp ../spark/jars/kryo-shaded-3.0.3.jar lib/cp ../spark/jars/minlog-1.3.0.jar lib/cp ../spark/jars/scala-xml_2.11-1.0.5.jar lib/cp ../spark/jars/spark-launcher_2.11-2.3.3.jar lib/cp ../spark/jars/spark-network-shuffle_2.11-2.3.3.jar lib/cp ../spark/jars/spark-unsafe_2.11-2.3.3.jar lib/cp ../spark/jars/xbean-asm5-shaded-4.4.jar lib/ 修改配置 conf/hive-site.yml: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:21109/metastore?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.cj.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123123&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.execution.engine&lt;/name&gt; &lt;value&gt;spark&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.enable.spark.execution.engine&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;spark.yarn.jars&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000/spark-jars/*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;spark.master&lt;/name&gt; &lt;value&gt;spark://localhost.localdomain:7077&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;spark.eventLog.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;spark.eventLog.dir&lt;/name&gt; &lt;value&gt;file:///home/tools/hive/logs&lt;/value&gt; # &lt;/property&gt; &lt;property&gt; &lt;name&gt;spark.serializer&lt;/name&gt; &lt;value&gt;org.apache.spark.serializer.KryoSerializer&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 创建运行数据库实例的 docker-compose.yml : 12345678910version: \"2.4\"services:mysql: restart: always image: mysql:8 container_name: mysql4hive ports: - 21109:3306 environment: MYSQL_ROOT_PASSWORD: 123123 在 docker-compose.yml 所在目录运行: 1docker-compose up -d 初始化数据库，提示 schemaTool completed 则为成功，连接数据库实例，能看到 metastore 数据库和相关的表 1/home/tools/hive/bin/schematool -initSchema -dbType mysql 修改 /etc/profile 配置环境变量 1234export HIVE_HOME=/home/tools/hiveexport SPARK_HOME=/home/tools/sparkexport HADOOP_HOME=/home/tools/hadoopexport PATH=$HIVE_HOME/bin:$PATH 添加文件用来插入假数据 test_data.txt: 121 Bryant2 Jordan 运行Spark1/home/tools/spark/sbin/start-all.sh Hive123456789101112131415161718192021222324root@localhost: /home/tools/hive/bin/beeline -u jdbc:hive2://hive &gt; create table test2 (id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ';hive &gt; load data local inpath '/home/tools/hive/test_data.txt' into table test2;hive &gt; select count(*) from test2;Hive on Spark Session Web UI URL: http://192.168.99.100:4040Query Hive on Spark job[0] stages: [0, 1]Spark job[0] status = RUNNING-------------------------------------------------------------------------------------- STAGES ATTEMPT STATUS TOTAL COMPLETED RUNNING PENDING FAILED --------------------------------------------------------------------------------------Stage-0 ........ 0 FINISHED 1 1 0 0 0 Stage-1 ........ 0 FINISHED 1 1 0 0 0 --------------------------------------------------------------------------------------STAGES: 02/02 [==========================&gt;&gt;] 100% ELAPSED TIME: 13.05 s --------------------------------------------------------------------------------------Spark job[0] finished successfully in 13.05 second(s)OK+------+| _c0 |+------+| 2 |+------+1 row selected (60.643 seconds) 打完收工 ヾ(￣▽￣)Bye~Bye~","categories":[],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://pyun.top/tags/Hive/"},{"name":"Spark","slug":"Spark","permalink":"https://pyun.top/tags/Spark/"}]},{"title":"Spring Boot + Jaeger 搭建使用","slug":"jaeger-deploy","date":"2019-05-06T03:31:15.000Z","updated":"2019-05-06T03:55:13.194Z","comments":true,"path":"2019/jaeger-deploy-with-spring-boot/","link":"","permalink":"https://pyun.top/2019/jaeger-deploy-with-spring-boot/","excerpt":"","text":"基于 Docker 简单搭建 Jaeger 的示例，此例子运行在虚拟机中，虚拟机的 IP 为 192.168.99.100 Jaeger 搭建创建数据库 Cassandra使用的 docker-compose 文件如下:123456789version: \"3\"services: cassandra: container_name: jaeger_db_cassandra image: cassandra:3 volumes: - /var/lib/cassandra ports: - 9042:9042 使用 Jaeger 提供的镜像为 Cassandra 初始化数据库1docker run --link jaeger_db_cassandra:cassandra --net cassandra_default --rm -ti jaegertracing/jaeger-cassandra-schema 因为 Cassandra 用了 Docker-Compose 启动，所以初始化时要用 --net 参数连接到 Cassandra 所在的网络 启动 Jaeger 的 Query 、 Collector 和 Agent 服务使用的 docker-compose 文件如下:123456789101112131415161718192021222324252627282930313233version: \"3\"services: query: container_name: jaeger_query image: jaegertracing/jaeger-query:1.11.0 environment: - SPAN_STORAGE_TYPE=cassandra - CASSANDRA_KEYSPACE=jaeger_v1_dc1 - CASSANDRA_SERVERS=192.168.99.100 ports: - 16686:16686/tcp collector: container_name: jaeger_collector image: jaegertracing/jaeger-collector:1.11.0 environment: - SPAN_STORAGE_TYPE=cassandra - CASSANDRA_KEYSPACE=jaeger_v1_dc1 - CASSANDRA_SERVERS=192.168.99.100 ports: - 9411:9411/tcp - 14267:14267/tcp - 14268:14268/tcp agent: container_name: jaeger_agent image: jaegertracing/jaeger-agent:1.11.0 environment: - COLLECTOR_HOST_PORT=192.168.99.100:14267 ports: - 5775:5775/udp - 6831-6832:6831-6832/udp - 5778:5778/tcp depends_on: - collector 每个组件可配置的参数可运行对应的镜像 + -h 参数查看，如：1docker run --rm -ti jaegertracing/jaeger-collector -h 用 Docker-Compose 运行 Jaeger 时，参数除了配置成环境变量，也可以配置成启动参数的形式，对应的环境变量的名称就是启动的参数去掉 -- ，字母大写，’.‘ 转换成 ‘_‘，如：12345678version: \"3\"services: query: container_name: jaeger_query image: jaegertracing/jaeger-query:1.11.0 ports: - 16686:16686/tcp command: \"--cassandra.keyspace jaeger_v1_dc1 --cassandra.servers 192.168.99.100\" 示例代码首先构造一个项目，作为前端，用户访问前端的地址时，前端会调用一次微服务，并返回页面给用户用 IDEA 新建项目，选择 Spring Initializr ，后面的依赖中选择 Web 就行 build.gradle 中引入 Jaeger Client 和发起 Http 调用的 okhttp ，如下所示：123456789101112131415161718192021222324252627282930313233343536plugins &#123; id 'org.springframework.boot' version '2.1.4.RELEASE' id 'java'&#125;apply plugin: 'io.spring.dependency-management'bootJar &#123; baseName = 'frontend'&#125;repositories &#123; mavenCentral()&#125;sourceCompatibility = '11'ext &#123; set('springCloudVersion', 'Greenwich.SR1')&#125;dependencies &#123; implementation 'io.jaegertracing:jaeger-client:0.34.0' implementation 'com.squareup.okhttp3:okhttp:3.4.2' implementation 'org.springframework.boot:spring-boot-starter-web' implementation 'org.springframework.boot:spring-boot-starter-thymeleaf' testImplementation 'org.springframework.boot:spring-boot-starter-test'&#125;dependencyManagement &#123; imports &#123; mavenBom \"org.springframework.cloud:spring-cloud-dependencies:$&#123;springCloudVersion&#125;\" &#125;&#125; 注册 Tracer 的 Bean ，为了方便这里直接放在框架启动类 Application 中123456789101112131415161718192021@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125; @Bean public Tracer tracer() &#123; SamplerConfiguration samplerConfig = SamplerConfiguration.fromEnv() .withType(ConstSampler.TYPE) .withParam(1); ReporterConfiguration reporterConfig = ReporterConfiguration.fromEnv() .withLogSpans(true); Configuration config = new Configuration(\"frontend-demo\") .withSampler(samplerConfig) .withReporter(reporterConfig); return config.getTracer(); &#125;&#125; 控制器中调用别的服务时，将其包装为 Span ，为了能构造调用的上下级关系，把本次调用的 Span 的上下文注入（Inject）到 Tracer 中，如下所示：123456789101112131415161718192021222324252627282930313233@Controllerpublic class FrontendController &#123; @Autowired private Tracer tracer; @RequestMapping(\"/\") public String index(Model model) throws IOException, InterruptedException &#123; Scope span = tracer.buildSpan(\"frontend-index\").startActive(true); String response = callExternalService(\"eureka-hello-service\"); span.close(); model.addAttribute(\"data\", response); return \"index\"; &#125; private String callExternalService(String name) throws IOException, InterruptedException &#123; String url = \"http://192.168.99.100:21001/greeting\"; Request.Builder requestBuilder = new Request.Builder().url(url); tracer.inject(tracer.activeSpan().context(), Format.Builtin.HTTP_HEADERS, new RequestBuilderCarrier(requestBuilder)); Request request = requestBuilder.build(); OkHttpClient client = new OkHttpClient(); try (Response response = client.newCall(request).execute()) &#123; return response.body().string(); &#125; &#125;&#125; 其中，注入 Span 上下文时用到的 RequestBuilderCarrier 可自定义，如：1234567891011121314151617181920212223import io.opentracing.propagation.TextMap;import okhttp3.Request;import java.util.Iterator;import java.util.Map;public class RequestBuilderCarrier implements TextMap &#123; private final Request.Builder requestBuilder; public RequestBuilderCarrier(Request.Builder requestBuilder) &#123; this.requestBuilder = requestBuilder; &#125; @Override public Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iterator() &#123; throw new UnsupportedOperationException(\"carrier is writer-only\"); &#125; @Override public void put(String key, String value) &#123; requestBuilder.addHeader(key, value); &#125;&#125; 同上，新建一个 Spring Boot 项目，作为被调用的微服务，Jaeger 配置相同，控制器如下：1234567891011121314151617181920212223@RestControllerpublic class GreetingController &#123; private static final String template = \"Hello, %s!\"; private final AtomicLong counter = new AtomicLong(); @Autowired private Tracer tracer; @RequestMapping(\"/greeting\") public Greeting greeting(@RequestParam(value=\"name\", defaultValue=\"World\") String name, @RequestHeader HttpHeaders headers) throws IOException, InterruptedException &#123; SpanContext spanContext = tracer.extract(Format.Builtin.HTTP_HEADERS, new TextMapExtractAdapter(headers.toSingleValueMap())); Span span = tracer.buildSpan(\"test\").asChildOf(spanContext).start(); span.finish(); return new Greeting(counter.incrementAndGet(), String.format(template, name)); &#125;&#125; 启动示例代码首先，在两个项目中分别运行 gradle build 生成 Jar 文件，这里是 frontend.jar 和 ms-service.jar ，把它们跟docker-compose.yml文件放一起，docker-compose.yml 如下：12345678910111213141516171819202122version: \"3\"services: frontend: container_name: frontend_1 image: adoptopenjdk/openjdk11:jdk-11.0.2.9 volumes: - ./frontend.jar:/frontend.jar command: java -jar /frontend.jar ports: - 21000:8080 environment: - JAEGER_AGENT_HOST=192.168.99.100 msservice: container_name: ms_service_1 image: adoptopenjdk/openjdk11:jdk-11.0.2.9 volumes: - ./ms-service.jar:/ms-service.jar command: java -jar /ms-service.jar ports: - 21001:8080 environment: - JAEGER_AGENT_HOST=192.168.99.100 注意代码跟 docker-compose.yml 中的地址、端口的对应关系用 Docker-Compose 启动项目，在浏览器中访问前端地址 192.168.99.100:21000，获取到返回的页面后，访问 Jaeger Query 的地址 192.168.99.100:16686，左侧栏选择前端服务，点击 Find Traces，即可看到调用链信息","categories":[],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://pyun.top/tags/Spring-Boot/"},{"name":"Jaeger","slug":"Jaeger","permalink":"https://pyun.top/tags/Jaeger/"}]},{"title":"搭建虚拟机Docker开发环境","slug":"vmware-setup","date":"2019-05-05T02:43:54.185Z","updated":"2019-05-05T08:13:02.798Z","comments":true,"path":"2019/setup-docker-develop-environment-based-on-vmware/","link":"","permalink":"https://pyun.top/2019/setup-docker-develop-environment-based-on-vmware/","excerpt":"","text":"宿主机是 Win 10，需要使用 Docker 作为开发环境的话，使用 VMWare 安装 Linux 虚拟机，如 Ubuntu 18 ，然后在虚拟机里面安装 Docker Quick Start虚拟机安装新建虚拟机，基本各种默认就行，网络使用nat 网络设置 设置网段VMWare -&gt; 菜单 -&gt; 编辑 -&gt; 虚拟网络编辑器 中配置子网IP和和子网掩码，点击进去 NAT设置 可配置网关IP 配置静态IPUbuntu 中修改 /etc/netplan/50-cloud-init.yaml 配置 1234567891011network: ethernets: ens32: addresses: [192.168.99.100/24] dhcp4: no gateway4: 192.168.99.2 nameservers: addresses: - 119.29.29.29 - 223.5.5.5 version: 2 addresses 是要配置的静态IPgateway4 是前一步配置的网关IPnameservers 下面是 DNS 地址配置，如果SSH连接慢的话，可以试一下将 DNS 的第一项改成前一步的网关IP然后使用1netplan apply 重启网络服务即可。其它系统根据实际不同做相应的配置 Win 10 的网络连接里面，配置 VMware Network Adapter VMnet8 的IP，点击属性 -&gt; TCP/IPV4 -&gt; 属性，配置 IP 地址和子网掩码即可 挂载文件简单点的可以使用 VMWare 自带的共享文件夹，可参考别的文章，这里使用 cifs 方式挂载 Win 10 选择要挂载的文件夹，右键 属性 -&gt; 共享 ，进去设置一下 Ubuntu 里面修改 /etc/fstab 添加一条挂载设置1//192.168.99.1/workspace /work cifs vers=2.1,username=myname,password=mypwd,file_mode=0777,dir_mode=0777,noperm 0 0 这些配置的意思是： 192.168.99.1：网络设置第3步为宿主机设置的IP workspace：Win 10 里面要挂载的、被共享的文件夹名称 /work：共享文件夹被映射到 Linux 中的这个路径 myname 和 mypwd：Win 10 的用户名和密码 其它的是一些文件夹权限的设置 请根据实际情况修改运行指令 mount -a，正常的话文件夹就被挂载进来了，这里可能需要根据提示装一些挂载格式支持的软件。重启虚拟机，文件夹会自动挂载，不用每次都跑这条命令。 Docker 和 Docker Compose 安装根据文档来就行","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://pyun.top/tags/Docker/"},{"name":"VMWare","slug":"VMWare","permalink":"https://pyun.top/tags/VMWare/"}]}]}