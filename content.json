{"meta":{"title":"Pengyun's Blog","subtitle":"ლ(ٱ٥ٱლ)","description":"学习的路上，不时写点总结","author":"pengyun","url":"https://pyun.top","root":"/"},"pages":[{"title":"标签","date":"2019-03-26T09:12:11.232Z","updated":"2019-03-26T09:12:11.232Z","comments":false,"path":"tags/index.html","permalink":"https://pyun.top/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2019-03-26T07:08:55.000Z","updated":"2019-03-26T09:51:42.337Z","comments":true,"path":"categories/index.html","permalink":"https://pyun.top/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Hive on Spark 搭建","slug":"hive-on-spark-deploy","date":"2019-06-10T13:59:43.000Z","updated":"2019-06-10T14:32:24.907Z","comments":true,"path":"2019/running-hive-on-spark/","link":"","permalink":"https://pyun.top/2019/running-hive-on-spark/","excerpt":"","text":"Hive on Spark 就是把 Hive 默认的执行引擎从 MapReduce 换成 Apache Spark。Hive on Spark 只在 Spark 的制定版本上进行了测试，其它版本则不是很有保证。对稳定性有要求可以去官方文档查看版本兼容性，再根据 Hive 和 Spark 源码中 pom 文件的配置，另行选择版本。这里几乎全部选择最新版进行演示。 环境 VMWare 15，参考之前的搭建博文 Ubuntu 18.04 bionic 8G 内存 Docker 18.09.0 全程 root 用户操作（手动滑稽） 准备 安装Docker，拉取 MySQL 8 镜像（不使用 Docker 的后面自行安装 MySQL） 下载安装包 AdoptOpenJDK 8 Maven 3.6.1 Hadoop 3.1.2 Hive 3.1.1 Scala 2.11.12 Mysql Connector 8.0.16 Spark 二进制包可以在这里获取，但是我们要自己编译不内建 Hive 的版本，所以要下载源码编译 12git clone -b v2.3.3 https://github.com/apache/spark.gitrm -rf spark/.git 创建 /home/tools 文件夹，将解压后的安装包和源码都放这里，解压后的 mysql-connector-java-8.0.16.jar 放到 /home/tools/hive/lib 12345678910home | -- tools | -- hadoop | -- hive | -- lib | -- mysql-connector-java-8.0.16.jar | -- java | -- maven | -- scala | -- spark-source 安装Java、Maven 和 Scala 安装 编辑 /etc/profile，加入环境变量 123456 export JAVA_HOME=/home/tools/java export M2_HOME=/home/tools/maven export SCALA_HOME=/home/tools/scala export PATH=$&#123;M2_HOME&#125;/bin:$&#123;SCALA_HOME&#125;/bin:$&#123;JAVA_HOME&#125;/bin:$PATH ``` - 在终端运行以下指令，若显示出 jdk、Scala 和 Maven 版本，则安装已完成 source /etc/profile java -version scala -version mvn -v 123- （可选）网络不好的孩纸可自行配置 Maven 的仓库为阿里云的镜像### Hadoop 安装 - 安装需要的软件 apt install -y ssh pdsh rsync 12345678910111213141516171819202122- 修改配置 - `etc/hadoop/core-site.xml`: ``` xml &lt;configuration&gt; &lt;property&gt; name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/tmp/data&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; ``` - `etc/hadoop/hdfs-site.xml`: ``` xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; etc/hadoop/mapred-site.xml: 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; etc/hadoop/yarn-site.xml: 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; sbin/start-dfs.sh、sbin/stop-dfs.sh 头部配置使用 root 账户: 12345#!/usr/bin/env bashHDFS_DATANODE_USER=rootHDFS_DATANODE_SECURE_USER=hdfsHDFS_NAMENODE_USER=rootHDFS_SECONDARYNAMENODE_USER=root sbin/start-yarn.sh、sbin/stop-yarn.sh 头部配置使用 root 账户: 1234#!/usr/bin/env bashYARN_RESOURCEMANAGER_USER=rootHADOOP_SECURE_DN_USER=yarnYARN_NODEMANAGER_USER=root etc/hadoop/hadoop-env.sh 中添加: 12export PDSH_RCMD_TYPE=sshexport JAVA_HOME=/home/tools/java 配置 SSH 密钥登录 123ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsacat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keyschmod 0600 ~/.ssh/authorized_keys 修改 SSH 配置文件 /etc/ssh/sshd_config，加入: 12PermitRootLogin yes PubkeyAuthentication yes 重启 SSH 服务: 1service ssh restart 运行 ssh localhost 测试，看是否直接登录到本机 格式化文件系统 1bin/hdfs namenode -format 运行 HDFS，运行成功后可访问 http://localhost:9870 查看 NameNode 的 Web 管理界面 1sbin/start-dfs.sh 运行 yarn，运行成功后可访问 http://localhost:8088 查看 ResourceManager 的 Web 管理界面 1sbin/start-yarn.sh Spark 安装 编译不带 Hive 的版本 12345cd /home/tools/spark-source./dev/make-distribution.sh --name \"hadoop3-without-hive\" --tgz \"-Pyarn,-Phadoop-3.1,-Dhadoop.version=3.1.2,parquet-provided,orc-provided\"cd ..mkdir sparktar -xf spark-source/spark-2.3.3-bin-hadoop3-without-hive.tgz --strip-components 1 -C spark 新建配置文件 /home/tools/spark/conf/spark-env.sh: 1234#!/usr/bin/env bashexport JAVA_HOME=/home/tools/javaexport HADOOP_CONF_DIR=/home/tools/hadoop/etc/hadoopexport SPARK_DIST_CLASSPATH=$(/home/tools/hadoop/bin/hadoop classpath) Hive 安装 复制 Jar 包到 Hive 的 lib 文件夹，其中某些包的小版本号可能会不一样 1234567891011121314cd /home/tools/hivecp ../scala/lib/scala-library.jar lib/cp ../spark/jars/spark-core_2.11-2.3.3.jar lib/cp ../spark/jars/spark-network-common_2.11-2.3.3.jar lib/cp ../spark/jars/jersey-container-servlet-core-2.22.2.jar lib/cp ../spark/jars/jersey-server-2.22.2.jar lib/cp ../spark/jars/json4s-ast_2.11-3.2.11.jar lib/cp ../spark/jars/kryo-shaded-3.0.3.jar lib/cp ../spark/jars/minlog-1.3.0.jar lib/cp ../spark/jars/scala-xml_2.11-1.0.5.jar lib/cp ../spark/jars/spark-launcher_2.11-2.3.3.jar lib/cp ../spark/jars/spark-network-shuffle_2.11-2.3.3.jar lib/cp ../spark/jars/spark-unsafe_2.11-2.3.3.jar lib/cp ../spark/jars/xbean-asm5-shaded-4.4.jar lib/ 修改配置 conf/hive-site.yml，并复制到 $SPARK_HOME/conf: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:21109/metastore?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.cj.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123123&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.execution.engine&lt;/name&gt; &lt;value&gt;spark&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.enable.spark.execution.engine&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;spark.yarn.jars&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000/spark-jars/*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;spark.master&lt;/name&gt; &lt;value&gt;spark://localhost.localdomain:7077&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;spark.eventLog.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;spark.eventLog.dir&lt;/name&gt; &lt;value&gt;file:///home/tools/hive/logs&lt;/value&gt; # &lt;/property&gt; &lt;property&gt; &lt;name&gt;spark.serializer&lt;/name&gt; &lt;value&gt;org.apache.spark.serializer.KryoSerializer&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 创建运行数据库实例的 docker-compose.yml : 12345678910version: \"2.4\"services:mysql: restart: always image: mysql:8 container_name: mysql4hive ports: - 21109:3306 environment: MYSQL_ROOT_PASSWORD: 123123 在 docker-compose.yml 所在目录运行: 1docker-compose up -d 初始化数据库，提示 schemaTool completed 则为成功，连接数据库实例，能看到 metastore 数据库和相关的表 1/home/tools/hive/bin/schematool -initSchema -dbType mysql 修改 /etc/profile 配置环境变量 1234export HIVE_HOME=/home/tools/hiveexport SPARK_HOME=/home/tools/sparkexport HADOOP_HOME=/home/tools/hadoopexport PATH=$HIVE_HOME/bin:$PATH 添加文件用来插入假数据 test_data.txt: 121 Bryant2 Jordan 运行Spark1/home/tools/spark/sbin/start-all.sh Hive123456789101112131415161718192021222324root@localhost: /home/tools/hive/bin/beeline -u jdbc:hive2://hive &gt; create table test2 (id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ';hive &gt; load data local inpath '/home/tools/hive/test_data.txt' into table test2;hive &gt; select count(*) from test2;Hive on Spark Session Web UI URL: http://192.168.99.100:4040Query Hive on Spark job[0] stages: [0, 1]Spark job[0] status = RUNNING-------------------------------------------------------------------------------------- STAGES ATTEMPT STATUS TOTAL COMPLETED RUNNING PENDING FAILED --------------------------------------------------------------------------------------Stage-0 ........ 0 FINISHED 1 1 0 0 0 Stage-1 ........ 0 FINISHED 1 1 0 0 0 --------------------------------------------------------------------------------------STAGES: 02/02 [==========================&gt;&gt;] 100% ELAPSED TIME: 13.05 s --------------------------------------------------------------------------------------Spark job[0] finished successfully in 13.05 second(s)OK+------+| _c0 |+------+| 2 |+------+1 row selected (60.643 seconds) 打完收工 ヾ(￣▽￣)Bye~Bye~","categories":[],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://pyun.top/tags/Hive/"},{"name":"Spark","slug":"Spark","permalink":"https://pyun.top/tags/Spark/"}]},{"title":"Spring Boot + Jaeger 搭建使用","slug":"jaeger-deploy","date":"2019-05-06T03:31:15.000Z","updated":"2019-05-06T03:55:13.194Z","comments":true,"path":"2019/jaeger-deploy-with-spring-boot/","link":"","permalink":"https://pyun.top/2019/jaeger-deploy-with-spring-boot/","excerpt":"","text":"基于 Docker 简单搭建 Jaeger 的示例，此例子运行在虚拟机中，虚拟机的 IP 为 192.168.99.100 Jaeger 搭建创建数据库 Cassandra使用的 docker-compose 文件如下:123456789version: \"3\"services: cassandra: container_name: jaeger_db_cassandra image: cassandra:3 volumes: - /var/lib/cassandra ports: - 9042:9042 使用 Jaeger 提供的镜像为 Cassandra 初始化数据库1docker run --link jaeger_db_cassandra:cassandra --net cassandra_default --rm -ti jaegertracing/jaeger-cassandra-schema 因为 Cassandra 用了 Docker-Compose 启动，所以初始化时要用 --net 参数连接到 Cassandra 所在的网络 启动 Jaeger 的 Query 、 Collector 和 Agent 服务使用的 docker-compose 文件如下:123456789101112131415161718192021222324252627282930313233version: \"3\"services: query: container_name: jaeger_query image: jaegertracing/jaeger-query:1.11.0 environment: - SPAN_STORAGE_TYPE=cassandra - CASSANDRA_KEYSPACE=jaeger_v1_dc1 - CASSANDRA_SERVERS=192.168.99.100 ports: - 16686:16686/tcp collector: container_name: jaeger_collector image: jaegertracing/jaeger-collector:1.11.0 environment: - SPAN_STORAGE_TYPE=cassandra - CASSANDRA_KEYSPACE=jaeger_v1_dc1 - CASSANDRA_SERVERS=192.168.99.100 ports: - 9411:9411/tcp - 14267:14267/tcp - 14268:14268/tcp agent: container_name: jaeger_agent image: jaegertracing/jaeger-agent:1.11.0 environment: - COLLECTOR_HOST_PORT=192.168.99.100:14267 ports: - 5775:5775/udp - 6831-6832:6831-6832/udp - 5778:5778/tcp depends_on: - collector 每个组件可配置的参数可运行对应的镜像 + -h 参数查看，如：1docker run --rm -ti jaegertracing/jaeger-collector -h 用 Docker-Compose 运行 Jaeger 时，参数除了配置成环境变量，也可以配置成启动参数的形式，对应的环境变量的名称就是启动的参数去掉 -- ，字母大写，’.‘ 转换成 ‘_‘，如：12345678version: \"3\"services: query: container_name: jaeger_query image: jaegertracing/jaeger-query:1.11.0 ports: - 16686:16686/tcp command: \"--cassandra.keyspace jaeger_v1_dc1 --cassandra.servers 192.168.99.100\" 示例代码首先构造一个项目，作为前端，用户访问前端的地址时，前端会调用一次微服务，并返回页面给用户用 IDEA 新建项目，选择 Spring Initializr ，后面的依赖中选择 Web 就行 build.gradle 中引入 Jaeger Client 和发起 Http 调用的 okhttp ，如下所示：123456789101112131415161718192021222324252627282930313233343536plugins &#123; id 'org.springframework.boot' version '2.1.4.RELEASE' id 'java'&#125;apply plugin: 'io.spring.dependency-management'bootJar &#123; baseName = 'frontend'&#125;repositories &#123; mavenCentral()&#125;sourceCompatibility = '11'ext &#123; set('springCloudVersion', 'Greenwich.SR1')&#125;dependencies &#123; implementation 'io.jaegertracing:jaeger-client:0.34.0' implementation 'com.squareup.okhttp3:okhttp:3.4.2' implementation 'org.springframework.boot:spring-boot-starter-web' implementation 'org.springframework.boot:spring-boot-starter-thymeleaf' testImplementation 'org.springframework.boot:spring-boot-starter-test'&#125;dependencyManagement &#123; imports &#123; mavenBom \"org.springframework.cloud:spring-cloud-dependencies:$&#123;springCloudVersion&#125;\" &#125;&#125; 注册 Tracer 的 Bean ，为了方便这里直接放在框架启动类 Application 中123456789101112131415161718192021@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125; @Bean public Tracer tracer() &#123; SamplerConfiguration samplerConfig = SamplerConfiguration.fromEnv() .withType(ConstSampler.TYPE) .withParam(1); ReporterConfiguration reporterConfig = ReporterConfiguration.fromEnv() .withLogSpans(true); Configuration config = new Configuration(\"frontend-demo\") .withSampler(samplerConfig) .withReporter(reporterConfig); return config.getTracer(); &#125;&#125; 控制器中调用别的服务时，将其包装为 Span ，为了能构造调用的上下级关系，把本次调用的 Span 的上下文注入（Inject）到 Tracer 中，如下所示：123456789101112131415161718192021222324252627282930313233@Controllerpublic class FrontendController &#123; @Autowired private Tracer tracer; @RequestMapping(\"/\") public String index(Model model) throws IOException, InterruptedException &#123; Scope span = tracer.buildSpan(\"frontend-index\").startActive(true); String response = callExternalService(\"eureka-hello-service\"); span.close(); model.addAttribute(\"data\", response); return \"index\"; &#125; private String callExternalService(String name) throws IOException, InterruptedException &#123; String url = \"http://192.168.99.100:21001/greeting\"; Request.Builder requestBuilder = new Request.Builder().url(url); tracer.inject(tracer.activeSpan().context(), Format.Builtin.HTTP_HEADERS, new RequestBuilderCarrier(requestBuilder)); Request request = requestBuilder.build(); OkHttpClient client = new OkHttpClient(); try (Response response = client.newCall(request).execute()) &#123; return response.body().string(); &#125; &#125;&#125; 其中，注入 Span 上下文时用到的 RequestBuilderCarrier 可自定义，如：1234567891011121314151617181920212223import io.opentracing.propagation.TextMap;import okhttp3.Request;import java.util.Iterator;import java.util.Map;public class RequestBuilderCarrier implements TextMap &#123; private final Request.Builder requestBuilder; public RequestBuilderCarrier(Request.Builder requestBuilder) &#123; this.requestBuilder = requestBuilder; &#125; @Override public Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iterator() &#123; throw new UnsupportedOperationException(\"carrier is writer-only\"); &#125; @Override public void put(String key, String value) &#123; requestBuilder.addHeader(key, value); &#125;&#125; 同上，新建一个 Spring Boot 项目，作为被调用的微服务，Jaeger 配置相同，控制器如下：1234567891011121314151617181920212223@RestControllerpublic class GreetingController &#123; private static final String template = \"Hello, %s!\"; private final AtomicLong counter = new AtomicLong(); @Autowired private Tracer tracer; @RequestMapping(\"/greeting\") public Greeting greeting(@RequestParam(value=\"name\", defaultValue=\"World\") String name, @RequestHeader HttpHeaders headers) throws IOException, InterruptedException &#123; SpanContext spanContext = tracer.extract(Format.Builtin.HTTP_HEADERS, new TextMapExtractAdapter(headers.toSingleValueMap())); Span span = tracer.buildSpan(\"test\").asChildOf(spanContext).start(); span.finish(); return new Greeting(counter.incrementAndGet(), String.format(template, name)); &#125;&#125; 启动示例代码首先，在两个项目中分别运行 gradle build 生成 Jar 文件，这里是 frontend.jar 和 ms-service.jar ，把它们跟docker-compose.yml文件放一起，docker-compose.yml 如下：12345678910111213141516171819202122version: \"3\"services: frontend: container_name: frontend_1 image: adoptopenjdk/openjdk11:jdk-11.0.2.9 volumes: - ./frontend.jar:/frontend.jar command: java -jar /frontend.jar ports: - 21000:8080 environment: - JAEGER_AGENT_HOST=192.168.99.100 msservice: container_name: ms_service_1 image: adoptopenjdk/openjdk11:jdk-11.0.2.9 volumes: - ./ms-service.jar:/ms-service.jar command: java -jar /ms-service.jar ports: - 21001:8080 environment: - JAEGER_AGENT_HOST=192.168.99.100 注意代码跟 docker-compose.yml 中的地址、端口的对应关系用 Docker-Compose 启动项目，在浏览器中访问前端地址 192.168.99.100:21000，获取到返回的页面后，访问 Jaeger Query 的地址 192.168.99.100:16686，左侧栏选择前端服务，点击 Find Traces，即可看到调用链信息","categories":[],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://pyun.top/tags/Spring-Boot/"},{"name":"Jaeger","slug":"Jaeger","permalink":"https://pyun.top/tags/Jaeger/"}]},{"title":"搭建虚拟机Docker开发环境","slug":"vmware-setup","date":"2019-05-05T02:43:54.185Z","updated":"2019-05-05T08:13:02.798Z","comments":true,"path":"2019/setup-docker-develop-environment-based-on-vmware/","link":"","permalink":"https://pyun.top/2019/setup-docker-develop-environment-based-on-vmware/","excerpt":"","text":"宿主机是 Win 10，需要使用 Docker 作为开发环境的话，使用 VMWare 安装 Linux 虚拟机，如 Ubuntu 18 ，然后在虚拟机里面安装 Docker Quick Start虚拟机安装新建虚拟机，基本各种默认就行，网络使用nat 网络设置 设置网段VMWare -&gt; 菜单 -&gt; 编辑 -&gt; 虚拟网络编辑器 中配置子网IP和和子网掩码，点击进去 NAT设置 可配置网关IP 配置静态IPUbuntu 中修改 /etc/netplan/50-cloud-init.yaml 配置 1234567891011network: ethernets: ens32: addresses: [192.168.99.100/24] dhcp4: no gateway4: 192.168.99.2 nameservers: addresses: - 119.29.29.29 - 223.5.5.5 version: 2 addresses 是要配置的静态IPgateway4 是前一步配置的网关IPnameservers 下面是 DNS 地址配置，如果SSH连接慢的话，可以试一下将 DNS 的第一项改成前一步的网关IP然后使用1netplan apply 重启网络服务即可。其它系统根据实际不同做相应的配置 Win 10 的网络连接里面，配置 VMware Network Adapter VMnet8 的IP，点击属性 -&gt; TCP/IPV4 -&gt; 属性，配置 IP 地址和子网掩码即可 挂载文件简单点的可以使用 VMWare 自带的共享文件夹，可参考别的文章，这里使用 cifs 方式挂载 Win 10 选择要挂载的文件夹，右键 属性 -&gt; 共享 ，进去设置一下 Ubuntu 里面修改 /etc/fstab 添加一条挂载设置1//192.168.99.1/workspace /work cifs vers=2.1,username=myname,password=mypwd,file_mode=0777,dir_mode=0777,noperm 0 0 这些配置的意思是： 192.168.99.1：网络设置第3步为宿主机设置的IP workspace：Win 10 里面要挂载的、被共享的文件夹名称 /work：共享文件夹被映射到 Linux 中的这个路径 myname 和 mypwd：Win 10 的用户名和密码 其它的是一些文件夹权限的设置 请根据实际情况修改运行指令 mount -a，正常的话文件夹就被挂载进来了，这里可能需要根据提示装一些挂载格式支持的软件。重启虚拟机，文件夹会自动挂载，不用每次都跑这条命令。 Docker 和 Docker Compose 安装根据文档来就行","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://pyun.top/tags/Docker/"},{"name":"VMWare","slug":"VMWare","permalink":"https://pyun.top/tags/VMWare/"}]}]}